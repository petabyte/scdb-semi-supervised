{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel, BertConfig, BertForSequenceClassification\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786\n"
     ]
    }
   ],
   "source": [
    "labeled_test_df = pd.read_pickle('test_data_unseen_bert.zip', compression='zip')\n",
    "print(len(labeled_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_list = labeled_test_df['summary'].tolist()\n",
    "test_labels = np.array(labeled_test_df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'scdb_train_tuned_model'\n",
    "num_labels = 14\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=False)\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Petit', '##ione', '##r', 'might', 'have', 'been', 'fishing', 'at', 'a', 'location', 'outside', 'the', 'boundaries', 'of', 'what', 'is', ',', 'or', 'was', ',', 'the', 'P', '##uy', '##all', '##up', 'Indian', 'Reservation', 'when', 'the', 'acts', 'with', 'which', 'he', 'is', 'charged', 'were', 'committed', '.', 'If', 'this', 'were', 'so', ',', 'the', 'Supreme', 'Court', 'of', 'Washington', 'then', 'un', '##ne', '##cess', '##arily', 'addressed', ',', 'and', 'determined', ',', 'the', 'federal', 'question', 'whether', 'the', 'Reservation', '\"', 'has', 'ceased', 'to', 'exist', '\"', 'The', 'petition', 'for', 'a', 'w', '##rit', 'of', 'c', '##ert', '##ior', '##ari', 'is', 'granted', ',', 'the', 'judgment', 'of', 'the', 'Supreme', 'court', 'of', 'Washington', 'is', 'vacated', ',', 'and', 'the', 'case', 'is', 're', '##mand', '##ed', 'to', 'that', 'court', 'for', 'resolution', 'by', 'the', 'state', 'courts', 'of', 'the', 'fact', '##ual', 'issue', 'whether', 'the', 'alleged', 'offense', '##s', 'took', 'place', 'outside', 'what', 'is', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Default MAX sequence length for BertModel\n",
    "MAX_SEQ_LENGTH = 128\n",
    "def pad_sequences(pad_token, seq_list, max_length):\n",
    "    return seq_list + [pad_token] * (max_length - len(seq_list))\n",
    "\n",
    "def pad_special_tokens(tokenized_text_sent):\n",
    "    if len(tokenized_text_sent) > MAX_SEQ_LENGTH - 2:\n",
    "           tokenized_text_sent = tokenized_text_sent[0:(MAX_SEQ_LENGTH - 2)]            \n",
    "    tokenized_text_sent.insert(0,'[CLS]')\n",
    "    tokenized_text_sent.append('[SEP]')\n",
    "    return tokenized_text_sent\n",
    "\n",
    "def tokenize_sentence(summary_text): \n",
    "    tokenized_text_sent = tokenizer.tokenize(summary_text[0]['summary_text'])                  \n",
    "    tokenized_text_sent = pad_special_tokens(tokenized_text_sent)\n",
    "    return tokenized_text_sent  \n",
    "\n",
    "tokenized_seq = list(map(tokenize_sentence, test_text_list))\n",
    "print(tokenized_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101, 21386, 23997,  1197,  1547,  1138,  1151,  5339,  1120,   170,\n",
      "          2450,  1796,  1103,  7070,  1104,  1184,  1110,   117,  1137,  1108,\n",
      "           117,  1103,   153, 20257,  5727,  4455,  1890, 21542,  1165,  1103,\n",
      "          4096,  1114,  1134,  1119,  1110,  4601,  1127,  4762,   119,  1409,\n",
      "          1142,  1127,  1177,   117,  1103,  3732,  2031,  1104,  1994,  1173,\n",
      "          8362,  1673, 22371, 18206,  7894,   117,  1105,  3552,   117,  1103,\n",
      "          2877,  2304,  2480,  1103, 21542,   107,  1144,  6445,  1106,  4056,\n",
      "           107,  1109, 10077,  1111,   170,   192,  7729,  1104,   172,  7340,\n",
      "         18472,  7710,  1110,  3609,   117,  1103,  9228,  1104,  1103,  3732,\n",
      "          2175,  1104,  1994,  1110, 16216,   117,  1105,  1103,  1692,  1110,\n",
      "          1231, 20993,  1174,  1106,  1115,  2175,  1111,  6021,  1118,  1103,\n",
      "          1352,  5333,  1104,  1103,  1864,  4746,  2486,  2480,  1103,  6351,\n",
      "         10027,  1116,  1261,  1282,  1796,  1184,  1110,   102]],\n",
      "       device='cuda:0'), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "def convert_tokens_to_tensor(tokenized_sentence_a):        \n",
    "    tokenized_text = []\n",
    "    tokenized_text.extend(tokenized_sentence_a)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    indexed_tokens = pad_sequences(0, indexed_tokens, MAX_SEQ_LENGTH)    \n",
    "    tokens_tensor = torch.tensor([indexed_tokens], device=device)\n",
    "    #generate the token type ids\n",
    "    token_type_ids = []\n",
    "    token_type_a = [0] * len(tokenized_sentence_a)\n",
    "    token_type_ids.extend(token_type_a)\n",
    "    token_type_ids = pad_sequences(0, token_type_ids, MAX_SEQ_LENGTH)    \n",
    "    token_type_tensor = torch.tensor([token_type_ids],device=device)\n",
    "    #generate the type ids\n",
    "    input_mask = [1] * len(tokenized_text)\n",
    "    input_mask = pad_sequences(0, input_mask, MAX_SEQ_LENGTH)    \n",
    "    input_tensor = torch.tensor([input_mask],device=device)\n",
    "    return [tokens_tensor, token_type_tensor, input_tensor]    \n",
    "\n",
    "converted_tensors = list(map(convert_tokens_to_tensor, tokenized_seq))\n",
    "print(converted_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, trange\n",
    "\n",
    "predictions = []\n",
    "for predict_record in converted_tensors:\n",
    "    outputs = model(predict_record[0], token_type_ids=predict_record[1], attention_mask=predict_record[2])\n",
    "    prediction = np.argmax(outputs[0][0].cpu().detach().numpy())\n",
    "    predictions.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.87      0.83       401\n",
      "         2.0       0.59      0.61      0.60        70\n",
      "         3.0       0.68      0.63      0.65        70\n",
      "         4.0       0.22      0.22      0.22        27\n",
      "         5.0       1.00      0.67      0.80         3\n",
      "         6.0       1.00      0.29      0.44         7\n",
      "         7.0       0.25      0.17      0.20         6\n",
      "         8.0       0.66      0.59      0.62        68\n",
      "         9.0       0.40      0.29      0.34        76\n",
      "        10.0       0.42      0.31      0.36        16\n",
      "        11.0       0.81      0.81      0.81        16\n",
      "        12.0       0.75      0.72      0.73        25\n",
      "        13.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69       786\n",
      "   macro avg       0.58      0.48      0.51       786\n",
      "weighted avg       0.68      0.69      0.68       786\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(test_labels, predictions)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82742317 0.6013986  0.65185185 0.22222222 0.8        0.44444444\n",
      " 0.2        0.62015504 0.33587786 0.35714286 0.8125     0.73469388\n",
      " 0.         0.50828538 0.68322421]\n",
      "[401  70  70  27   3   7   6  68  76  16  16  25   1 786 786]\n",
      "0.6249112636494086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py:498: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"_draw_bs_pairs\" failed type inference due to: \u001b[1m\u001b[1m\u001b[1mInvalid use of type(CPUDispatcher(<function _make_two_arg_numba_func.<locals>.f at 0x000001709C22DC18>)) with parameters (array(float64, 1d, C), array(float64, 1d, C), Tuple())\n",
      " * parameterized\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: resolving callee type: type(CPUDispatcher(<function _make_two_arg_numba_func.<locals>.f at 0x000001709C22DC18>))\u001b[0m\n",
      "\u001b[0m\u001b[1m[2] During: typing of call at c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py (510)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py\", line 510:\u001b[0m\n",
      "\u001b[1m    def _draw_bs_pairs(x, y):\n",
      "        <source elided>\n",
      "            bs_x, bs_y = x[bs_inds], y[bs_inds]\n",
      "\u001b[1m            bs_replicates[i] = f(bs_x, bs_y, args)\n",
      "\u001b[0m            \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @numba.jit\n",
      "c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py:498: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"_draw_bs_pairs\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py\", line 507:\u001b[0m\n",
      "\u001b[1m    def _draw_bs_pairs(x, y):\n",
      "        <source elided>\n",
      "        # Generate replicates\n",
      "\u001b[1m        for i in range(size):\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @numba.jit\n",
      "c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\numba\\core\\object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"_draw_bs_pairs\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"..\\..\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py\", line 501:\u001b[0m\n",
      "\u001b[1m    def _draw_bs_pairs(x, y):\n",
      "        <source elided>\n",
      "        # Set up array of indices to sample from\n",
      "\u001b[1m        inds = np.arange(n)\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\numba\\core\\object_mode_passes.py:188: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py\", line 501:\u001b[0m\n",
      "\u001b[1m    def _draw_bs_pairs(x, y):\n",
      "        <source elided>\n",
      "        # Set up array of indices to sample from\n",
      "\u001b[1m        inds = np.arange(n)\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py:498: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"_draw_bs_pairs\" failed type inference due to: \u001b[1m\u001b[1m\u001b[1mInvalid use of type(CPUDispatcher(<function _make_two_arg_numba_func.<locals>.f at 0x000001709C22DC18>)) with parameters (array(float64, 1d, C), array(float64, 1d, C), Tuple())\n",
      " * parameterized\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: resolving callee type: type(CPUDispatcher(<function _make_two_arg_numba_func.<locals>.f at 0x000001709C22DC18>))\u001b[0m\n",
      "\u001b[0m\u001b[1m[2] During: typing of call at c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py (510)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py\", line 510:\u001b[0m\n",
      "\u001b[1m    def _draw_bs_pairs(x, y):\n",
      "        <source elided>\n",
      "            bs_x, bs_y = x[bs_inds], y[bs_inds]\n",
      "\u001b[1m            bs_replicates[i] = f(bs_x, bs_y, args)\n",
      "\u001b[0m            \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @numba.jit\n",
      "c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\numba\\core\\object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"_draw_bs_pairs\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"..\\..\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py\", line 507:\u001b[0m\n",
      "\u001b[1m    def _draw_bs_pairs(x, y):\n",
      "        <source elided>\n",
      "        # Generate replicates\n",
      "\u001b[1m        for i in range(size):\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "c:\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\numba\\core\\object_mode_passes.py:188: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\users\\u0155811\\.conda\\envs\\tf-gpu\\lib\\site-packages\\dc_stat_think\\dc_stat_think.py\", line 507:\u001b[0m\n",
      "\u001b[1m    def _draw_bs_pairs(x, y):\n",
      "        <source elided>\n",
      "        # Generate replicates\n",
      "\u001b[1m        for i in range(size):\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50287998 0.73899758]\n",
      "0.6250523507036786\n",
      "0.0006169434784094495\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ast\n",
    "from scipy import stats\n",
    "import dc_stat_think as dcst\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Add the label 13 which was missed in the splits\n",
    "f1_score_list = []\n",
    "support =[]\n",
    "cr_dict = classification_report(test_labels, predictions, output_dict=True)\n",
    "for cr_value_dict in cr_dict.values():\n",
    "    if isinstance(cr_value_dict, dict):\n",
    "        f1_score_list.append(cr_value_dict['f1-score'])\n",
    "        support.append(cr_value_dict['support'])\n",
    "f1_score_np = np.array(f1_score_list)\n",
    "support_np = np.array(support)\n",
    "#Bootstrap sampling to calculate the confidence interval for f1-score\n",
    "def weighted_average(x, y):\n",
    "    return np.sum(x * y)/np.sum(y)\n",
    "\n",
    "def boostrap_weighted_avg(data,size):\n",
    "    return dcst.draw_bs_pairs(data, support, weighted_average, size=size)\n",
    "   \n",
    "print(f1_score_np)    \n",
    "print(support_np)\n",
    "print(weighted_average(f1_score_np, support_np))\n",
    "bs_weighted_avg = boostrap_weighted_avg(f1_score_np, 10000)\n",
    "print(np.percentile(bs_weighted_avg, [2.5, 97.5]))\n",
    "print(np.mean(bs_weighted_avg))\n",
    "print(stats.sem(bs_weighted_avg, axis=None, ddof=0))\n",
    "plt.hist(bs_weighted_avg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
